"""
LLM Service - Handles integration with various Language Models
"""

import os
import json
import logging
import httpx
import asyncio
from typing import Dict, Any, List, Optional

# Import the enhanced environment loader
from .enhanced_env_loader import get_api_key_for_provider, get_llm_config_from_env
from ..config import get_settings

# Configure logging
logger = logging.getLogger(__name__)

class LLMService:
    """Service for interacting with language models"""
    
    def __init__(self):
        """Initialize LLM service"""
        self.settings = get_settings()
        self.config = self.settings.llm
        self.initialized = False
        self.providers = {}
        
        # Ensure API key is loaded from environment if needed
        if not self.config.get("api_key") and self.config.get("provider") != "ollama":
            provider = self.config.get("provider", "ollama")
            api_key = get_api_key_for_provider(provider)
            if api_key:
                self.config["api_key"] = api_key
                logger.info(f"Loaded API key for {provider} from environment")
        
        logger.info(f"ðŸ” LLM config: type={self.config.get('type', 'local')}, provider={self.config.get('provider', 'ollama')}, model={self.config.get('model', 'llama3.2:latest')}")
        logger.info(f"LLM Service initialized with {self.config.get('provider', 'ollama')} ({self.config.get('type', 'local')})")
        
    async def initialize(self):
        """Initialize connections to LLM providers"""
        if self.initialized:
            return
        
        try:
            # Get available providers
            await self._discover_providers()
            self.initialized = True
        except Exception as e:
            logger.error(f"Failed to initialize LLM service: {str(e)}")
            raise
    
    async def _discover_providers(self):
        """Discover available LLM providers"""
        # Initialize basic provider info
        self.providers = {
            "ollama": {
                "name": "Ollama",
                "is_local": True,
                "base_url": "http://127.0.0.1:11434",
                "models": []
            }
        }
        
        # Get available Ollama models
        try:
            if "ollama" in self.providers:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.get(f"{self.providers['ollama']['base_url']}/api/tags")
                    if response.status_code == 200:
                        data = response.json()
                        if "models" in data:
                            self.providers["ollama"]["models"] = [
                                {"id": model["name"], "name": model["name"]}
                                for model in data["models"]
                            ]
                        else:
                            # Handle 0.5.0+ Ollama API response
                            self.providers["ollama"]["models"] = [
                                {"id": model["name"], "name": model["name"]}
                                for model in data.get("models", [])
                            ]

            # Add Anthropic provider
            self.providers["anthropic"] = {
                "name": "Anthropic",
                "is_local": False,
                "base_url": "https://api.anthropic.com",
                "models": [
                    {"id": "claude-3-sonnet-20240229", "name": "Claude 3 Sonnet"},
                    {"id": "claude-3-opus-20240229", "name": "Claude 3 Opus"},
                    {"id": "claude-3-haiku-20240307", "name": "Claude 3 Haiku"},
                    {"id": "claude-3.5-sonnet-20250626", "name": "Claude 3.5 Sonnet"}
                ]
            }
            
            # Add OpenAI provider
            self.providers["openai"] = {
                "name": "OpenAI",
                "is_local": False,
                "base_url": "https://api.openai.com",
                "models": [
                    {"id": "gpt-4o", "name": "GPT-4o"},
                    {"id": "gpt-4", "name": "GPT-4"},
                    {"id": "gpt-3.5-turbo", "name": "GPT-3.5 Turbo"}
                ]
            }
            
            # Add Hunyuan3D provider (updated for fal.ai)
            self.providers["hunyuan3d"] = {
                "name": "Hunyuan3D",
                "is_local": False,
                "base_url": "https://api.fal.ai",
                "models": [
                    {"id": "fal-ai/hunyuan3d/v2/multi-view", "name": "Hunyuan-3D Base"},
                    {"id": "fal-ai/hunyuan3d/v2/multi-view-hd", "name": "Hunyuan-3D HD"}
                ]
            }
            
        except Exception as e:
            logger.warning(f"Failed to get Ollama models: {str(e)}")
            # Add default models as fallback
            self.providers["ollama"]["models"] = [
                {"id": "llama3.2:latest", "name": "Llama 3.2"},
                {"id": "llama3:latest", "name": "Llama 3"},
                {"id": "llama2:latest", "name": "Llama 2"}
            ]
    
    def get_providers(self) -> List[Dict[str, Any]]:
        """Get list of available LLM providers and models"""
        providers_list = []
        
        for provider_id, provider_info in self.providers.items():
            providers_list.append({
                "name": provider_info["name"],
                "is_local": provider_info["is_local"],
                "models": provider_info["models"]
            })
        
        return providers_list
    
    async def generate(
        self, 
        prompt: str, 
        provider: Optional[str] = None,
        model: Optional[str] = None,
        parameters: Optional[Dict[str, Any]] = None
    ) -> str:
        """Generate text from a prompt using the specified LLM"""
        if not self.initialized:
            await self.initialize()
        
        # Use parameters from request or fallback to defaults
        provider = provider or self.config.get("provider", "ollama")
        model = model or self.config.get("model", "llama3:latest")
        parameters = parameters or {}
        
        # Set default parameters if not provided
        if "temperature" not in parameters:
            parameters["temperature"] = 0.7
        if "max_tokens" not in parameters:
            parameters["max_tokens"] = 2048
        
        # Generate based on provider
        if provider.lower() == "ollama":
            return await self._generate_ollama(prompt, model, parameters)
        elif provider.lower() == "anthropic":
            return await self._generate_anthropic(prompt, model, parameters)
        elif provider.lower() == "openai":
            return await self._generate_openai(prompt, model, parameters)
        elif provider.lower() == "hunyuan3d":
            return await self._generate_hunyuan3d(prompt, model, parameters)
        else:
            raise ValueError(f"Unsupported provider: {provider}")
    
    async def _generate_ollama(self, prompt: str, model: str, parameters: Dict[str, Any]) -> str:
        """Generate text using Ollama API"""
        base_url = self.providers.get("ollama", {}).get("base_url", "http://127.0.0.1:11434")
        
        # Map our generic parameters to Ollama specific ones
        ollama_params = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": parameters.get("temperature", 0.7),
                "num_predict": parameters.get("max_tokens", 2048)
            }
        }
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{base_url}/api/generate",
                    json=ollama_params
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get("response", "")
                else:
                    error_msg = f"Ollama API error: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return f"Error: {error_msg}. Please make sure Ollama is running and the model is installed."
        except httpx.ConnectError:
            error_msg = f"Could not connect to Ollama at {base_url}. Please make sure Ollama is running."
            logger.error(error_msg)
            return error_msg
        except Exception as e:
            error_msg = f"Error generating text with Ollama: {str(e)}"
            logger.error(error_msg)
            return f"Error: {error_msg}"
    
    async def _generate_anthropic(self, prompt: str, model: str, parameters: Dict[str, Any]) -> str:
        """Generate text using Anthropic API"""
        # Try to get API key from environment first, then config
        api_key = get_api_key_for_provider("anthropic") or self.config.get("api_key")
        
        if not api_key:
            error_msg = "Anthropic API key not found. Set ANTHROPIC_API_KEY environment variable or configure in settings."
            logger.error(error_msg)
            return error_msg
        
        # Map our generic parameters to Anthropic specific ones
        max_tokens = parameters.get("max_tokens", 2048)
        temperature = parameters.get("temperature", 0.7)
        
        # Updated headers with correct format
        headers = {
            "Content-Type": "application/json",
            "X-API-Key": api_key,  # Changed to X-API-Key (capital X)
            "anthropic-version": "2023-06-01"
        }
        
        # First try the newer Messages API format
        try:
            # Messages API format (newer and recommended)
            messages_body = {
                "model": model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": max_tokens,
                "temperature": temperature
            }
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.anthropic.com/v1/messages",
                    headers=headers,
                    json=messages_body
                )
                
                if response.status_code == 200:
                    data = response.json()
                    logger.debug(f"Claude API response: {data}")
                    data = response.json()
                    # Extract the message content from the response
                    if "content" in data and len(data["content"]) > 0:
                        # Messages API returns an array of content blocks
                        content_blocks = data["content"]
                        text_blocks = [block["text"] for block in content_blocks if block["type"] == "text"]
                        return "".join(text_blocks)
                    return "".join(text_blocks)
                    return ""
                else:
                    error_msg = f"Anthropic API error: {response.status_code} - {response.text}"
                    logger.error(error_msg)
        except Exception as e:
            logger.warning(f"Messages API failed, falling back to Completion API: {str(e)}")
            # Fall back to the older Completion API
        
        # Completion API format (older)
        completion_body = {
            "model": model,
            "prompt": f"\n\nHuman: {prompt}\n\nAssistant:",
            "max_tokens_to_sample": max_tokens,
            "temperature": temperature
        }
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.anthropic.com/v1/complete",
                    headers=headers,
                    json=completion_body
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get("completion", "")
                else:
                    error_msg = f"Anthropic API error: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return f"Error: {error_msg}"
        except Exception as e:
            error_msg = f"Error generating text with Anthropic: {str(e)}"
            logger.error(error_msg)
            return f"Error: {error_msg}"
            
    async def _generate_openai(self, prompt: str, model: str, parameters: Dict[str, Any]) -> str:
        """Generate text using OpenAI API"""
        # Try to get API key from environment first, then config
        api_key = get_api_key_for_provider("openai") or self.config.get("api_key")
        
        if not api_key:
            error_msg = "OpenAI API key not found. Set OPENAI_API_KEY environment variable or configure in settings."
            logger.error(error_msg)
            return error_msg
        
        # Map our generic parameters to OpenAI specific ones
        max_tokens = parameters.get("max_tokens", 2048)
        temperature = parameters.get("temperature", 0.7)
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # Build request body for chat completion
        request_body = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers=headers,
                    json=request_body
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return data.get("choices", [{}])[0].get("message", {}).get("content", "")
                else:
                    error_msg = f"OpenAI API error: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return f"Error: {error_msg}"
        except Exception as e:
            error_msg = f"Error generating text with OpenAI: {str(e)}"
            logger.error(error_msg)
            return f"Error: {error_msg}"

    async def _generate_hunyuan3d(self, prompt: str, model: str, parameters: Dict[str, Any]) -> str:
        """Generate 3D content using Hunyuan3D API via fal.ai"""
        # Try to get API key from environment first, then config
        api_key = get_api_key_for_provider("hunyuan3d") or self.config.get("api_key")
        
        if not api_key:
            error_msg = "Hunyuan3D API key not found. Set HUNYUAN3D_API_KEY environment variable or configure in settings."
            logger.error(error_msg)
            return error_msg
        
        # fal.ai uses a different authentication method
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Key {api_key}"  # Note the format is "Key" not "Bearer"
        }
        
        # Build request body for the Hunyuan3D API on fal.ai
        # See https://fal.ai/models/fal-ai/hunyuan3d/v2/multi-view/api
        request_body = {
            "prompt": prompt,
            "negative_prompt": parameters.get("negative_prompt", ""),
            "num_inference_steps": parameters.get("num_inference_steps", 30),
            "guidance_scale": parameters.get("guidance_scale", 7.5),
            "width": parameters.get("width", 1024),
            "height": parameters.get("height", 1024),
            "seed": parameters.get("seed", None)
        }
        
        try:
            # Using the correct endpoint for fal.ai
            base_url = self.providers.get("hunyuan3d", {}).get("base_url", "https://api.fal.ai")
            endpoint = f"/models/{model}/infer"
            
            async with httpx.AsyncClient(timeout=120.0) as client:  # Longer timeout for 3D generation
                response = await client.post(
                    f"{base_url}{endpoint}",
                    headers=headers,
                    json=request_body
                )
                
                if response.status_code == 200:
                    data = response.json()
                    
                    # For text response in UI, provide the URLs to the generated content
                    result = "Hunyuan3D Generation Results:\n\n"
                    
                    if "images" in data:
                        result += "Generated images:\n"
                        for i, image_url in enumerate(data["images"], 1):
                            result += f"{i}. {image_url}\n"
                    
                    if "rendered_frames" in data:
                        result += "\nRendered frames:\n"
                        for i, frame in enumerate(data["rendered_frames"], 1):
                            result += f"{i}. {frame}\n"
                    
                    if "3d_model" in data:
                        result += f"\n3D Model: {data['3d_model']}\n"
                    
                    if "mesh_url" in data:
                        result += f"\nMesh URL: {data['mesh_url']}\n"
                        
                    return result
                else:
                    error_msg = f"Hunyuan3D API error: {response.status_code} - {response.text}"
                    logger.error(error_msg)
                    return f"Error: {error_msg}"
        except Exception as e:
            error_msg = f"Error generating content with Hunyuan3D: {str(e)}"
            logger.error(error_msg)
            return f"Error: {error_msg}"
