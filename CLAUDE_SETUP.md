# Claude Integration for GenAI Agent 3D

This document provides information about the Claude integration in the GenAI Agent 3D project.

## Configuration Changes Made

The following changes have been made to integrate Claude as the default LLM provider:

1. **Environment Files Updated**:
   - Added Anthropic API key to both `.env` files
   - Set Claude as the default LLM provider
   
2. **LLM Service Updated**:
   - Added support for Anthropic's Claude models
   - Implemented the `_generate_anthropic` method to handle API requests to Claude
   
3. **API Routes Updated**:
   - Added Claude models to the list of available providers
   - Updated model selection UI to include all Claude models

## Available Claude Models

The following Claude models are now available:

- **Claude 3 Opus** (claude-3-opus-20240229) - The most capable Claude model
- **Claude 3 Sonnet** (claude-3-sonnet-20240229) - Balanced performance and speed
- **Claude 3 Haiku** (claude-3-haiku-20240307) - Fastest response time
- **Claude 3.5 Sonnet** (claude-3.5-sonnet-20250626) - Latest model with improved capabilities

## Using Claude

Claude is now set as the default LLM provider, so you don't need to do anything special to use it. When you start the application, it will automatically use Claude for all LLM operations.

### Testing Claude

To test if Claude is working correctly:

1. Restart the services (if you haven't already):
   ```
   python restart_claude.py
   ```
   
   Or use the batch file:
   ```
   restart_with_claude.bat
   ```

2. Open the web interface at http://localhost:3000

3. Go to the "LLM Test" page

4. Enter a prompt and click "Generate" - the response should come from Claude

## Switching Between LLM Providers

If you want to switch between different LLM providers (e.g., between Claude and Ollama):

1. **Via Environment Files**:
   Edit either `.env` file and update these values:
   ```
   LLM_PROVIDER=anthropic  # or ollama, openai, etc.
   LLM_MODEL=claude-3-sonnet-20240229  # or another model ID
   LLM_TYPE=cloud  # or local for Ollama
   ```

2. **Via Config File**:
   Edit `genai_agent_project/config.yaml` and update these values in the `llm` section:
   ```yaml
   llm:
     provider: anthropic  # or ollama, openai, etc.
     model: claude-3-sonnet-20240229  # or another model ID
     type: cloud  # or local for Ollama
   ```

3. **Via Web Interface**:
   In the LLM Test page, you can select different providers and models from the dropdown menu.

## Troubleshooting

If you encounter issues with the Claude integration:

1. **API Key Issues**:
   - Ensure your Anthropic API key is correct in both `.env` files and in the `config.yaml` file
   - Check that the API key is being properly loaded (look for any error messages in the console)

2. **Connection Issues**:
   - Make sure your system has internet access to reach the Anthropic API
   - Check if there are any firewall or network restrictions blocking API access

3. **Error Messages**:
   - If you see error messages about "Unsupported provider", restart the services to ensure the updated code is being used
   - Check the logs for specific error messages from the Anthropic API

4. **Failed Generation**:
   - If text generation fails, try with Ollama to see if it's a Claude-specific issue
   - Check if your prompt follows Anthropic's content policies

## API Usage and Costs

Note that using Claude models via the Anthropic API incurs costs based on your usage:

- **Input Tokens**: Charged per 1,000 tokens sent to the API
- **Output Tokens**: Charged per 1,000 tokens generated by the model

The current approximate costs per 1,000 tokens are:
- Claude 3 Opus: $0.03 input / $0.15 output
- Claude 3 Sonnet: $0.003 input / $0.015 output
- Claude 3 Haiku: $0.00025 input / $0.00125 output
- Claude 3.5 Sonnet: $0.005 input / $0.025 output

Monitor your usage through the Anthropic console to avoid unexpected charges.

## Further Configuration

For advanced configuration options:

1. **Modifying Prompt Templates**:
   Edit the `prompt_templates` section in `config.yaml` to customize how prompts are formatted for Claude

2. **Adjusting Timeouts**:
   Set `timeout` and `generation_timeout` in the `llm` section of `config.yaml` to change request timeouts

3. **Default Parameters**:
   You can modify the default temperature and other parameters in the LLM service code
